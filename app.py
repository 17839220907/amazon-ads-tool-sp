import streamlit as st
import pandas as pd
import io
import datetime

# ================= 1. ç½‘é¡µåŸºç¡€è®¾ç½® =================
st.set_page_config(page_title="äºšé©¬é€ŠSPå¹¿å‘Šè‡ªåŠ¨ç”Ÿæˆå·¥å…·", page_icon="ğŸš€", layout="wide")

st.title("ğŸš€ äºšé©¬é€ŠSPå¹¿å‘Šè‡ªåŠ¨ç”Ÿæˆå·¥å…·")
st.markdown("""
**ä½¿ç”¨è¯´æ˜ï¼š**
1. å‡†å¤‡å¥½ä½ çš„ Excel é…ç½®æ–‡ä»¶ï¼ˆæ ¼å¼éœ€ä¸ **å¹¿å‘Šè‡ªåŠ¨ç”Ÿæˆå·¥å…·.xlsx** ä¸€è‡´ï¼‰ã€‚
2. ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸Šä¼ æ–‡ä»¶ã€‚
3. ç³»ç»Ÿå°†è‡ªåŠ¨å¤„ç†å¹¶æä¾› CSV ä¸‹è½½ã€‚
""")


# ================= 2. æ ¸å¿ƒå·¥å…·å‡½æ•° (å®Œå…¨å¤ç”¨åŸé€»è¾‘) =================

def clean_df(df):
    """æ¸…æ´—DataFrameï¼šå»åˆ—åç©ºæ ¼ã€å»ç©ºè¡Œ"""
    if df is None: return None
    df.columns = [str(c).strip().replace('\ufeff', '') for c in df.columns]
    df.dropna(how='all', inplace=True)
    return df


def find_sheet_strict(xls, target_name):
    """
    ã€æ ¸å¿ƒä¿®æ­£ã€‘ç²¾å‡†æŸ¥æ‰¾ Sheet
    é€»è¾‘ï¼šå¿…é¡»å®Œå…¨ç›¸ç­‰ï¼ˆå¿½ç•¥å¤§å°å†™å’Œé¦–å°¾ç©ºæ ¼ï¼‰
    ç»ä¸å…è®¸ "Pro" åŒ¹é…åˆ° "Pro Max"
    """
    target = target_name.lower().strip()
    for sheet in xls.sheet_names:
        # åªæœ‰å½“åå­—å®Œå…¨ä¸€æ ·æ—¶æ‰è¿”å›
        if sheet.lower().strip() == target:
            return sheet
    return None


def get_col(row, col_list):
    """è·å–æ•°å€¼"""
    for col in col_list:
        if col in row and pd.notna(row[col]):
            return float(row[col])
    return 0.0


def get_str(row, col_list):
    """è·å–å­—ç¬¦ä¸²"""
    for col in col_list:
        if col in row and pd.notna(row[col]):
            val = str(row[col]).strip()
            if val.endswith('.0'): return val[:-2]
            return val
    return None


# ================= 3. ä¸»ç¨‹åºé€»è¾‘ (é€‚é…ç½‘é¡µç«¯) =================

uploaded_file = st.file_uploader("è¯·æ‹–æ‹½æˆ–é€‰æ‹© Excel æ–‡ä»¶ (.xlsx)", type=['xlsx'])

if uploaded_file:
    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    with st.spinner('æ­£åœ¨æé€Ÿå¤„ç†ä¸­ï¼Œè¯·ç¨å€™...'):
        try:
            # 1. è¯»å– Excel å¯¹è±¡
            xls = pd.ExcelFile(uploaded_file)


            # 2. è¯»å–æ ¸å¿ƒé…ç½® Sheet
            def find_config_sheet(keyword):
                for s in xls.sheet_names:
                    if keyword in s: return s
                return None


            s_demand = find_config_sheet("å¹¿å‘Šéœ€æ±‚")
            s_style = find_config_sheet("æ¬¾å¼å")
            s_model = find_config_sheet("å‹å·å")

            # æ£€æŸ¥å¿…è¦ Sheet
            if not all([s_demand, s_style, s_model]):
                st.error(f"âŒ Excel ç¼ºå°‘æ ¸å¿ƒ Sheetï¼\næ£€æµ‹ç»“æœï¼šå¹¿å‘Šéœ€æ±‚={s_demand}, æ¬¾å¼å={s_style}, å‹å·å={s_model}")
                st.stop()

            # è¯»å–æ•°æ®
            df_demand = clean_df(pd.read_excel(xls, sheet_name=s_demand))
            df_style = clean_df(pd.read_excel(xls, sheet_name=s_style))
            df_model = clean_df(pd.read_excel(xls, sheet_name=s_model))

            # 3. æ„å»ºæ•°æ®æ˜ å°„
            # (1) æ¬¾å¼æ˜ å°„
            if 'ç¼©å†™' not in df_style.columns or 'æ¬¾å¼å…¨ç§°' not in df_style.columns:
                st.error("âŒ [æ¬¾å¼è¡¨] ç¼ºå°‘ 'ç¼©å†™' æˆ– 'æ¬¾å¼å…¨ç§°' åˆ—")
                st.stop()
            style_abbr_map = dict(zip(df_style['ç¼©å†™'].astype(str).str.strip(), df_style['æ¬¾å¼å…¨ç§°']))

            style_root_map = {}
            if 'æ ¸å¿ƒè¯æ ¹' in df_style.columns:
                for _, row in df_style.iterrows():
                    if pd.notna(row['æ ¸å¿ƒè¯æ ¹']):
                        val = str(row['æ ¸å¿ƒè¯æ ¹']).replace('ï¼Œ', ',')
                        style_root_map[row['æ¬¾å¼å…¨ç§°']] = [x.strip() for x in val.split(',')]
            else:
                st.error("âŒ [æ¬¾å¼è¡¨] ç¼ºå°‘ 'æ ¸å¿ƒè¯æ ¹' åˆ—")
                st.stop()

            # (2) å‹å·æ˜ å°„
            model_abbr_map = dict(zip(df_model['ç¼©å†™'].astype(str).str.strip(), df_model['å‹å·å…¨ç§°']))
            model_name_to_abbr = dict(zip(df_model['å‹å·å…¨ç§°'], df_model['ç¼©å†™']))

            col_fid = 'å¯¹åº”è¯è¡¨æ ‡è¯†'
            if col_fid not in df_model.columns:
                for c in df_model.columns:
                    if 'è¯è¡¨æ ‡è¯†' in c: col_fid = c; break

            if col_fid not in df_model.columns:
                st.error("âŒ [å‹å·è¡¨] ç¼ºå°‘ 'å¯¹åº”è¯è¡¨æ ‡è¯†' åˆ—")
                st.stop()

            model_file_id_map = dict(zip(df_model['å‹å·å…¨ç§°'], df_model[col_fid]))

            # 4. è§£æå¹¿å‘Šéœ€æ±‚
            parsed_data = []
            logs = []  # è®°å½•æ—¥å¿—

            for idx, row in df_demand.iterrows():
                sku = str(row['SKU']).strip()
                found_model = None
                found_style = None

                # ä¼˜å…ˆåŒ¹é…é•¿å­—ç¬¦ä¸²
                for abbr in sorted(model_abbr_map.keys(), key=len, reverse=True):
                    if abbr in sku:
                        found_model = model_abbr_map[abbr]
                        break

                for abbr in sorted(style_abbr_map.keys(), key=len, reverse=True):
                    if abbr in sku:
                        found_style = style_abbr_map[abbr]
                        break

                if found_model and found_style:
                    parsed_data.append({
                        'sku': sku,
                        'bid': get_col(row, ['ç«ä»·']),
                        'budget': get_col(row, ['æ¯æ—¥é¢„ç®—']),
                        'start_date': get_str(row, ['å¼€å§‹æ—¥æœŸ', 'Start Date']),
                        'match': str(row.get('åŒ¹é…æ¨¡å¼', 'ç²¾å‡†')).strip(),
                        'top': get_col(row, ['é¦–é¡µä½ç½®æº¢ä»·%', 'é¦–é¡µæº¢ä»·%']),
                        'prod': get_col(row, ['å•†å“é¡µæº¢ä»·%', 'å•†å“é¡µä½ç½®æº¢ä»·%']),
                        'rest': get_col(row, ['å…¶ä½™ä½ç½®æº¢ä»·%', 'å…¶ä½™æº¢ä»·%']),
                        'model': found_model,
                        'style': found_style
                    })
                else:
                    logs.append(f"âš ï¸ è·³è¿‡æ— æ•ˆSKU: {sku}")

            if not parsed_data:
                st.warning("âŒ æœªè¯†åˆ«åˆ°ä»»ä½•æœ‰æ•ˆä»»åŠ¡ï¼Œè¯·æ£€æŸ¥è¡¨æ ¼å†…å®¹ã€‚")
                st.stop()

            # 5. ç”Ÿæˆå¹¿å‘Šæ•°æ®
            df_p = pd.DataFrame(parsed_data)
            grouped = df_p.groupby(['model', 'style'])

            output_rows = []
            report_rows = []

            for (model, style), group in grouped:
                abbr = model_name_to_abbr.get(model)
                roots = style_root_map.get(style)
                file_id = model_file_id_map.get(model)

                if not roots or not file_id:
                    continue

                # --- æ ¸å¿ƒï¼šç²¾å‡†è¯»å–å…³é”®è¯ Sheet ---
                target_sheet_name = f"å…³é”®è¯-{file_id}"
                kw_sheet = find_sheet_strict(xls, target_sheet_name)

                valid_keywords = []
                if kw_sheet:
                    df_kw = clean_df(pd.read_excel(xls, sheet_name=kw_sheet))
                    if 'åˆ†ç±»' in df_kw.columns and 'å…³é”®è¯' in df_kw.columns:
                        # æ’é™¤ç«å“
                        df_kw = df_kw[~df_kw['åˆ†ç±»'].astype(str).str.contains('å“ç‰Œ', na=False)]
                        # ç­›é€‰è¯æ ¹
                        for _, k_row in df_kw.iterrows():
                            cat = str(k_row['åˆ†ç±»']).strip()
                            if cat in roots:
                                valid_keywords.append(k_row['å…³é”®è¯'])
                        valid_keywords = list(set(valid_keywords))
                    else:
                        logs.append(f"âš ï¸ Sheet [{kw_sheet}] ç¼ºå°‘ 'åˆ†ç±»' æˆ– 'å…³é”®è¯' åˆ—")
                else:
                    logs.append(f"ğŸ”¸ è­¦å‘Š: æ‰¾ä¸åˆ° Sheet -> [{target_sheet_name}]")

                first = group.iloc[0]
                camp_name = f"{abbr}-{style}-SP"

                # --- å†™å…¥ä¸­æ–‡è¡Œ ---
                # 1. å¹¿å‘Šæ´»åŠ¨
                output_rows.append({
                    'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'å¹¿å‘Šæ´»åŠ¨', 'æ“ä½œ': 'åˆ›å»º',
                    'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name, 'å¹¿å‘Šæ´»åŠ¨åç§°': camp_name,
                    'æŠ•æ”¾ç±»å‹': 'æ‰‹åŠ¨', 'çŠ¶æ€': 'å·²å¯ç”¨',
                    'æ¯æ—¥é¢„ç®—': first['budget'],
                    'å¼€å§‹æ—¥æœŸ': first['start_date'],
                    'ç«ä»·æ–¹æ¡ˆ': 'å›ºå®šç«ä»·'
                })

                # 2. ç«ä»·è°ƒæ•´
                if first['top'] > 0:
                    output_rows.append(
                        {'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'ç«ä»·è°ƒæ•´', 'æ“ä½œ': 'åˆ›å»º', 'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name,
                         'å¹¿å‘Šä½': 'å¹¿å‘Šä½ï¼šæœç´¢ç»“æœé¦–é¡µé¦–ä½', 'ç™¾åˆ†æ¯”': first['top']})
                if first['prod'] > 0:
                    output_rows.append(
                        {'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'ç«ä»·è°ƒæ•´', 'æ“ä½œ': 'åˆ›å»º', 'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name,
                         'å¹¿å‘Šä½': 'å¹¿å‘Šä½ï¼šå•†å“é¡µé¢', 'ç™¾åˆ†æ¯”': first['prod']})
                if first['rest'] > 0:
                    output_rows.append(
                        {'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'ç«ä»·è°ƒæ•´', 'æ“ä½œ': 'åˆ›å»º', 'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name,
                         'å¹¿å‘Šä½': 'å¹¿å‘Šä½ï¼šæœç´¢ç»“æœçš„å…¶ä½™ä½ç½®', 'ç™¾åˆ†æ¯”': first['rest']})

                # 3. å¹¿å‘Šç»„ (å›ºå®šç«ä»·=1)
                output_rows.append({
                    'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'å¹¿å‘Šç»„', 'æ“ä½œ': 'åˆ›å»º',
                    'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name, 'å¹¿å‘Šç»„ç¼–å·': camp_name,
                    'å¹¿å‘Šç»„åç§°': camp_name, 'çŠ¶æ€': 'å·²å¯ç”¨',
                    'å¹¿å‘Šç»„é»˜è®¤ç«ä»·': 1
                })

                # 4. å•†å“å¹¿å‘Š
                skus_list = []
                for _, item in group.iterrows():
                    skus_list.append(item['sku'])
                    output_rows.append({
                        'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'å•†å“å¹¿å‘Š', 'æ“ä½œ': 'åˆ›å»º',
                        'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name, 'å¹¿å‘Šç»„ç¼–å·': camp_name,
                        'SKU': item['sku'], 'çŠ¶æ€': 'å·²å¯ç”¨'
                    })

                # 5. å…³é”®è¯ (ä»…æ­¤å¤„å¡«å‚è€ƒç«ä»·=1)
                for kw in valid_keywords:
                    output_rows.append({
                        'äº§å“': 'å•†å“æ¨å¹¿', 'å®ä½“å±‚çº§': 'å…³é”®è¯', 'æ“ä½œ': 'åˆ›å»º',
                        'å¹¿å‘Šæ´»åŠ¨ç¼–å·': camp_name, 'å¹¿å‘Šç»„ç¼–å·': camp_name,
                        'å…³é”®è¯æ–‡æœ¬': kw,
                        'åŒ¹é…ç±»å‹': first['match'],
                        'ç«ä»·': group['bid'].max(),
                        'çŠ¶æ€': 'å·²å¯ç”¨',
                        'å¹¿å‘Šç»„é»˜è®¤ç«ä»·ï¼ˆä»…ä¾›å‚è€ƒï¼‰': 1
                    })

                # è¯´æ˜ä¹¦
                report_rows.append({
                    'å¹¿å‘Šæ´»åŠ¨åç§°': camp_name,
                    'å‹å·': model, 'æ¬¾å¼': style,
                    'ç›®æ ‡Sheet': target_sheet_name,
                    'åŒ¹é…çŠ¶æ€': 'âœ… æˆåŠŸ' if kw_sheet else 'âŒ å¤±è´¥',
                    'æ ¸å¿ƒè¯æ ¹': str(roots),
                    'å…³é”®è¯æ•°': len(valid_keywords),
                    'SKUåˆ—è¡¨': " | ".join(skus_list)
                })

                logs.append(f"ğŸ‰ ç”Ÿæˆ: {camp_name} ({len(valid_keywords)} è¯)")

            # 6. è¾“å‡ºç»“æœ
            if output_rows:
                st.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(output_rows)} è¡Œæ•°æ®ï¼")

                # å®šä¹‰åˆ—é¡ºåº
                cols = ['äº§å“', 'å®ä½“å±‚çº§', 'æ“ä½œ', 'å¹¿å‘Šæ´»åŠ¨ç¼–å·', 'å¹¿å‘Šç»„ç¼–å·', 'å¹¿å‘Šæ´»åŠ¨åç§°', 'å¹¿å‘Šç»„åç§°',
                        'æŠ•æ”¾ç±»å‹', 'çŠ¶æ€', 'æ¯æ—¥é¢„ç®—', 'å¼€å§‹æ—¥æœŸ', 'ç«ä»·æ–¹æ¡ˆ',
                        'å¹¿å‘Šç»„é»˜è®¤ç«ä»·', 'å¹¿å‘Šç»„é»˜è®¤ç«ä»·ï¼ˆä»…ä¾›å‚è€ƒï¼‰',
                        'SKU', 'ç«ä»·', 'åŒ¹é…ç±»å‹', 'å…³é”®è¯æ–‡æœ¬', 'å¹¿å‘Šä½', 'ç™¾åˆ†æ¯”']

                df_out = pd.DataFrame(output_rows)
                for c in cols:
                    if c not in df_out.columns: df_out[c] = None

                # è½¬æ¢CSVæ ¼å¼ (UTF-8-SIG)
                csv_upload = df_out[cols].to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')

                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

                # ä¸‹è½½åŒº
                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½ã€ä¸Šä¼ è¡¨ã€‘(ä¸­æ–‡ç‰ˆ)",
                        data=csv_upload,
                        file_name=f"ã€ä¸­æ–‡ç‰ˆã€‘SPå¹¿å‘Šä¸Šä¼ è¡¨_{timestamp}.csv",
                        mime='text/csv'
                    )

                with col2:
                    if report_rows:
                        csv_report = pd.DataFrame(report_rows).to_csv(index=False, encoding='utf-8-sig').encode(
                            'utf-8-sig')
                        st.download_button(
                            label="ğŸ“„ ä¸‹è½½ã€è¯´æ˜ä¹¦ã€‘",
                            data=csv_report,
                            file_name=f"ã€è¯´æ˜ä¹¦ã€‘å¹¿å‘Šç”Ÿæˆè¯¦æƒ…_{timestamp}.csv",
                            mime='text/csv'
                        )
            else:
                st.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•æ•°æ®ã€‚")

            # æŠ˜å æ˜¾ç¤ºæ—¥å¿—
            with st.expander("æŸ¥çœ‹è¯¦ç»†è¿è¡Œæ—¥å¿—"):
                for log in logs:
                    st.text(log)

        except Exception as e:
            st.error(f"âŒ ç¨‹åºå‘ç”Ÿé”™è¯¯: {e}")
            import traceback

            st.text(traceback.format_exc())